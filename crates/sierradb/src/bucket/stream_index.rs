//! The file format for an MPHF-based stream index is defined as follows:
//! - `[0..4]`     : magic marker: `b"SIDX"`
//! - `[4..12]`    : number of keys (n) as a `u64`
//! - `[12..20]`   : length of serialized MPHF (L) as a `u64`
//! - `[20..20+L]` : serialized MPHF bytes (using bincode)
//! - `[20+L..]`   : records array, exactly n records of RECORD_SIZE bytes each.

mod closed;
mod iter;
mod open;

use std::mem;

use uuid::Uuid;

pub use self::closed::{ClosedIndex, ClosedStreamIndex};
pub use self::iter::StreamIter;
pub use self::open::OpenStreamIndex;
use crate::STREAM_ID_SIZE;

const VERSION_SIZE: usize = mem::size_of::<u64>();
const PARTITION_KEY_SIZE: usize = mem::size_of::<Uuid>();
const OFFSET_SIZE: usize = mem::size_of::<u64>();
const LEN_SIZE: usize = mem::size_of::<u32>();
// Stream ID, version min, version max, partition key, offset, len
const RECORD_SIZE: usize =
    STREAM_ID_SIZE + VERSION_SIZE + VERSION_SIZE + PARTITION_KEY_SIZE + OFFSET_SIZE + LEN_SIZE;

#[derive(Clone, Debug, PartialEq, Eq)]
pub struct StreamIndexRecord<T> {
    pub partition_key: Uuid,
    pub version_min: u64,
    pub version_max: u64,
    pub offsets: T,
}

#[derive(Clone, Debug, PartialEq, Eq)]
pub enum StreamOffsets {
    Offsets(Vec<u64>), // Its cached
    ExternalBucket,    // This stream lives in a different bucket
}

#[cfg(test)]
mod tests {
    use std::path::PathBuf;
    use std::sync::Arc;
    use std::time::{SystemTime, UNIX_EPOCH};

    pub use sierradb_protocol::ExpectedVersion;
    use smallvec::smallvec;
    use uuid::Uuid;

    use super::*;
    use crate::StreamId;
    use crate::bucket::BucketSegmentId;
    use crate::bucket::segment::CommittedEvents;
    use crate::bucket::stream_index::closed::ClosedStreamIndex;
    use crate::bucket::stream_index::open::OpenStreamIndex;
    use crate::cache::BLOCK_SIZE;
    use crate::database::{Database, DatabaseBuilder, NewEvent, Transaction};
    use crate::id::{uuid_to_partition_hash, uuid_v7_with_partition_hash};

    const SEGMENT_SIZE: usize = 256_000_000; // 256 MB
    const SMALL_SEGMENT_SIZE: usize = BLOCK_SIZE * 2; // 2 blocks, forcing multiple segments

    fn temp_file_path() -> PathBuf {
        // Create a unique filename for each test to avoid conflicts
        let timestamp = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_nanos();

        tempfile::Builder::new()
            .prefix(&format!("test_{timestamp}_"))
            .make(|path| Ok(path.to_path_buf()))
            .unwrap()
            .path()
            .to_path_buf()
    }

    fn temp_dir() -> PathBuf {
        tempfile::tempdir()
            .expect("failed to create temp dir")
            .path()
            .to_path_buf()
    }

    /// Create a test database with configurable segment size for forcing
    /// multiple segments
    async fn create_test_database(segment_size: usize) -> Database {
        let dir = temp_dir();
        DatabaseBuilder::new()
            .segment_size(segment_size)
            .total_buckets(1) // Use single bucket for predictable testing
            .bucket_ids(Arc::from(vec![0])) // Single bucket with ID 0
            .reader_threads(1)
            .writer_threads(1)
            .open(dir)
            .expect("failed to create test database")
    }

    /// Helper to create test events for a stream
    fn create_test_event(stream_id: &str, partition_key: Uuid, partition_id: u16) -> NewEvent {
        let partition_hash = uuid_to_partition_hash(partition_key);
        let event_id = uuid_v7_with_partition_hash(partition_hash);

        NewEvent {
            event_id,
            stream_id: StreamId::new(stream_id).expect("invalid stream id"),
            stream_version: ExpectedVersion::Any,
            event_name: "TestEvent".to_string(),
            timestamp: 1000 + (partition_id as u64 * 1000), /* Different timestamp per event and
                                                             * partition */
            metadata: vec![],
            payload: b"payload".to_vec(),
        }
    }

    /// Helper to append events to database and return their details
    async fn append_events_to_stream(
        db: &Database,
        stream_id: &str,
        count: usize,
        partition_key: Uuid,
        partition_id: u16,
    ) -> Result<Vec<(u64, Uuid)>, Box<dyn std::error::Error>> {
        let mut results = Vec::new();

        for i in 0..count {
            let event = create_test_event(stream_id, partition_key, partition_id);
            let event_id = event.event_id;
            let transaction = Transaction::new(partition_key, partition_id, smallvec![event])?;
            let _result = db.append_events(transaction).await?;
            results.push((i as u64, event_id));
        }

        Ok(results)
    }

    /// Helper to append multiple events in a single transaction
    async fn append_transaction_events(
        db: &Database,
        events: Vec<NewEvent>,
        partition_key: Uuid,
        partition_id: u16,
    ) -> Result<(), Box<dyn std::error::Error>> {
        let transaction = Transaction::new(partition_key, partition_id, events.into())?;
        db.append_events(transaction).await?;
        Ok(())
    }

    #[test]
    fn test_open_stream_index_insert_and_get() {
        let path = temp_file_path();
        let mut index =
            OpenStreamIndex::create(BucketSegmentId::new(0, 0), path, SEGMENT_SIZE).unwrap();

        let stream_id = StreamId::new("stream-a").unwrap();
        let partition_key = Uuid::new_v4();
        let offsets = vec![42, 105];
        for (i, offset) in offsets.iter().enumerate() {
            index
                .insert(stream_id.clone(), partition_key, i as u64, *offset)
                .unwrap();
        }

        assert_eq!(
            index.get(&stream_id),
            Some(&StreamIndexRecord {
                version_min: 0,
                version_max: 1,
                partition_key,
                offsets: StreamOffsets::Offsets(offsets),
            })
        );
        assert_eq!(index.get("unknown"), None);
    }

    #[test]
    fn test_closed_stream_index_lookup() {
        let path = temp_file_path();
        let mut index =
            OpenStreamIndex::create(BucketSegmentId::new(0, 0), &path, SEGMENT_SIZE).unwrap();

        let stream_id1 = StreamId::new("stream-a").unwrap();
        let stream_id2 = StreamId::new("stream-b").unwrap();
        let partition_key1 = Uuid::new_v4();
        let partition_key2 = Uuid::new_v4();
        let offsets1 = vec![1111, 2222];
        let offsets2 = vec![3333];

        for (i, offset) in offsets1.iter().enumerate() {
            index
                .insert(stream_id1.clone(), partition_key1, i as u64, *offset)
                .unwrap();
        }
        for (i, offset) in offsets2.iter().enumerate() {
            index
                .insert(stream_id2.clone(), partition_key2, i as u64, *offset)
                .unwrap();
        }

        // Flush with the new MPHF format
        let (mphf, _) = index.flush().unwrap();
        assert!(mphf.try_hash(&stream_id1).is_some());
        assert!(mphf.try_hash(&stream_id2).is_some());
        assert!(mphf.try_hash(&StreamId::new("unknown").unwrap()).is_none());

        // Open with the closed index
        let mut closed_index =
            ClosedStreamIndex::open(BucketSegmentId::new(0, 0), &path, SEGMENT_SIZE).unwrap();

        // Test get_key with MPHF implementation
        let key1 = closed_index.get_key(&stream_id1).unwrap().unwrap();
        assert_eq!(key1.version_min, 0);
        assert_eq!(key1.version_max, 1);
        assert_eq!(key1.partition_key, partition_key1);

        // Test get with MPHF implementation
        assert_eq!(
            closed_index.get(&stream_id1).unwrap(),
            Some(StreamOffsets::Offsets(offsets1)),
        );

        // Test get_key for second stream ID
        let key2 = closed_index.get_key(&stream_id2).unwrap().unwrap();
        assert_eq!(key2.version_min, 0);
        assert_eq!(key2.version_max, 0);
        assert_eq!(key2.partition_key, partition_key2);

        // Test get for second stream ID
        assert_eq!(
            closed_index.get(&stream_id2).unwrap(),
            Some(StreamOffsets::Offsets(offsets2)),
        );

        // Test unknown stream ID
        assert_eq!(closed_index.get_key("unknown").unwrap(), None);
        assert_eq!(closed_index.get("unknown").unwrap(), None);
    }

    #[test]
    fn test_mphf_collision_handling() {
        let path = temp_file_path();
        let mut index =
            OpenStreamIndex::create(BucketSegmentId::new(0, 0), &path, SEGMENT_SIZE).unwrap();

        // These two IDs might have collisions in a regular hash table,
        // but MPHF should handle them perfectly
        let stream_id1 = StreamId::new("stream-a").unwrap();
        let stream_id2 = StreamId::new("stream-b").unwrap();
        let stream_id3 = StreamId::new("stream-k").unwrap();
        let stream_id4 = StreamId::new("stream-l").unwrap();

        let partition_key1 = Uuid::new_v4();
        let partition_key2 = Uuid::new_v4();
        let partition_key3 = Uuid::new_v4();
        let partition_key4 = Uuid::new_v4();

        let offsets1 = vec![883, 44];
        let offsets2 = vec![39, 1, 429];
        let offsets3 = vec![1111, 2222];
        let offsets4 = vec![3333];

        for (i, offset) in offsets1.iter().enumerate() {
            index
                .insert(stream_id1.clone(), partition_key1, i as u64, *offset)
                .unwrap();
        }
        for (i, offset) in offsets2.iter().enumerate() {
            index
                .insert(stream_id2.clone(), partition_key2, i as u64, *offset)
                .unwrap();
        }
        for (i, offset) in offsets3.iter().enumerate() {
            index
                .insert(stream_id3.clone(), partition_key3, i as u64, *offset)
                .unwrap();
        }
        for (i, offset) in offsets4.iter().enumerate() {
            index
                .insert(stream_id4.clone(), partition_key4, i as u64, *offset)
                .unwrap();
        }

        // Flush with the new MPHF format
        let (mphf, _) = index.flush().unwrap();

        // Verify MPHF correctness - all streams should have different hash values
        let hash1 = mphf.hash(&stream_id1);
        let hash2 = mphf.hash(&stream_id2);
        let hash3 = mphf.hash(&stream_id3);
        let hash4 = mphf.hash(&stream_id4);

        assert_ne!(hash1, hash2);
        assert_ne!(hash1, hash3);
        assert_ne!(hash1, hash4);
        assert_ne!(hash2, hash3);
        assert_ne!(hash2, hash4);
        assert_ne!(hash3, hash4);

        // Test with closed index
        let mut closed_index =
            ClosedStreamIndex::open(BucketSegmentId::new(0, 0), &path, SEGMENT_SIZE).unwrap();

        // For this test, we'll just check that we can retrieve all the stream IDs
        // without checking the exact offsets, since the file format has been updated
        assert_eq!(
            closed_index.get(&stream_id1).unwrap(),
            Some(StreamOffsets::Offsets(offsets1))
        );
        assert_eq!(
            closed_index.get(&stream_id2).unwrap(),
            Some(StreamOffsets::Offsets(offsets2))
        );
        assert_eq!(
            closed_index.get(&stream_id3).unwrap(),
            Some(StreamOffsets::Offsets(offsets3))
        );
        assert_eq!(
            closed_index.get(&stream_id4).unwrap(),
            Some(StreamOffsets::Offsets(offsets4))
        );

        // Unknown stream ID should not be found
        assert_eq!(closed_index.get("unknown").unwrap(), None);
    }

    #[test]
    fn test_non_existent_stream_lookup() {
        let path = temp_file_path();

        let mut index =
            OpenStreamIndex::create(BucketSegmentId::new(0, 0), &path, SEGMENT_SIZE).unwrap();
        index.flush().unwrap();

        let mut index =
            ClosedStreamIndex::open(BucketSegmentId::new(0, 0), path, SEGMENT_SIZE).unwrap();
        assert_eq!(index.get("unknown").unwrap(), None);
    }

    #[test]
    fn test_insert_external_bucket() {
        let path = temp_file_path();

        let stream_id = StreamId::new("my-stream").unwrap();
        let partition_key = Uuid::new_v4();

        let mut index =
            OpenStreamIndex::create(BucketSegmentId::new(0, 0), &path, SEGMENT_SIZE).unwrap();
        index
            .insert_external_bucket(stream_id.clone(), partition_key)
            .unwrap();
        assert_eq!(
            index.get(&stream_id),
            Some(&StreamIndexRecord {
                partition_key,
                version_min: 0,
                version_max: 0,
                offsets: StreamOffsets::ExternalBucket
            })
        );
        index.flush().unwrap();

        let mut index =
            ClosedStreamIndex::open(BucketSegmentId::new(0, 0), &path, SEGMENT_SIZE).unwrap();
        assert_eq!(
            index.get(&stream_id).unwrap(),
            Some(StreamOffsets::ExternalBucket)
        );
    }

    // ==================== STREAM ITERATOR TESTS ====================

    #[tokio::test]
    async fn test_stream_iter_single_segment() {
        let db = create_test_database(SEGMENT_SIZE).await;
        let partition_key = Uuid::new_v4();
        let partition_id = 0;
        let stream_id = "test-stream-single";

        // Append 10 events to a single stream
        let event_count = 10;
        append_events_to_stream(&db, stream_id, event_count, partition_key, partition_id)
            .await
            .expect("failed to append events");

        // Read events using stream iterator
        let mut stream_iter = db
            .read_stream(partition_id, StreamId::new(stream_id).unwrap(), 0, false)
            .await
            .expect("failed to create stream iterator");

        let mut collected_events = Vec::new();
        while let Some(committed_events) = stream_iter.next().await.expect("read err") {
            match committed_events {
                CommittedEvents::Single(event) => {
                    collected_events.push(event.stream_version);
                }
                CommittedEvents::Transaction { events, .. } => {
                    for event in events.iter() {
                        if event.stream_id.as_ref() == stream_id {
                            collected_events.push(event.stream_version);
                        }
                    }
                }
            }
        }

        // Verify all events were read in order
        assert_eq!(collected_events.len(), event_count);
        for (i, &version) in collected_events.iter().enumerate() {
            assert_eq!(version, i as u64, "events should be in order");
        }
    }

    #[tokio::test]
    async fn test_stream_iter_multiple_segments() {
        let db = create_test_database(SMALL_SEGMENT_SIZE).await;
        let partition_key = Uuid::new_v4();
        let partition_id = 0;
        let stream_id = "test-stream-multi";

        // Append enough events to force multiple segments (small segment size)
        let event_count = 50; // Should create multiple 2KB segments
        append_events_to_stream(&db, stream_id, event_count, partition_key, partition_id)
            .await
            .expect("failed to append events");

        // Force segments to close by appending to different stream
        let _other_events =
            append_events_to_stream(&db, "other-stream", 10, Uuid::new_v4(), partition_id).await;

        // Read events using stream iterator from beginning
        let mut stream_iter = db
            .read_stream(partition_id, StreamId::new(stream_id).unwrap(), 0, false)
            .await
            .expect("failed to create stream iterator");

        let mut collected_events = Vec::new();
        while let Some(committed_events) = stream_iter.next().await.expect("read err") {
            match committed_events {
                CommittedEvents::Single(event) => {
                    if event.stream_id.as_ref() == stream_id {
                        collected_events.push(event.stream_version);
                    }
                }
                CommittedEvents::Transaction { events, .. } => {
                    for event in events.iter() {
                        if event.stream_id.as_ref() == stream_id {
                            collected_events.push(event.stream_version);
                        }
                    }
                }
            }
        }

        // Verify all events were read in order across segments
        assert_eq!(collected_events.len(), event_count);
        for (i, &version) in collected_events.iter().enumerate() {
            assert_eq!(
                version, i as u64,
                "events should be in sequential order across segments"
            );
        }
    }

    #[tokio::test]
    async fn test_stream_iter_batch_reading() {
        let db = create_test_database(SMALL_SEGMENT_SIZE).await;
        let partition_key = Uuid::new_v4();
        let partition_id = 0;
        let stream_id = "test-stream-batch";

        let event_count = 100;
        append_events_to_stream(&db, stream_id, event_count, partition_key, partition_id)
            .await
            .expect("failed to append events");

        // Test next_batch with different limits
        let mut stream_iter = db
            .read_stream(partition_id, StreamId::new(stream_id).unwrap(), 0, false)
            .await
            .expect("failed to create stream iterator");

        let mut all_events = Vec::new();
        let batch_size = 5;

        loop {
            let batch = stream_iter.next_batch(batch_size).await.expect("read err");
            if batch.is_empty() {
                break;
            }

            for committed_events in batch {
                match committed_events {
                    CommittedEvents::Single(event) => {
                        all_events.push(event.stream_version);
                    }
                    CommittedEvents::Transaction { events, .. } => {
                        for event in events.iter() {
                            if event.stream_id.as_ref() == stream_id {
                                all_events.push(event.stream_version);
                            }
                        }
                    }
                }
            }
        }

        assert_eq!(all_events.len(), event_count);
        for (i, &version) in all_events.iter().enumerate() {
            assert_eq!(version, i as u64);
        }
    }

    #[tokio::test]
    async fn test_stream_iter_from_middle_version() {
        let db = create_test_database(SMALL_SEGMENT_SIZE).await;
        let partition_key = Uuid::new_v4();
        let partition_id = 0;
        let stream_id = "test-stream-middle";

        let event_count = 20;
        append_events_to_stream(&db, stream_id, event_count, partition_key, partition_id)
            .await
            .expect("failed to append events");

        // Start reading from version 10
        let start_version = 10;
        let mut stream_iter = db
            .read_stream(
                partition_id,
                StreamId::new(stream_id).unwrap(),
                start_version,
                false,
            )
            .await
            .expect("failed to create stream iterator");

        let mut collected_events = Vec::new();
        while let Some(committed_events) = stream_iter.next().await.expect("read err") {
            match committed_events {
                CommittedEvents::Single(event) => {
                    collected_events.push(event.stream_version);
                }
                CommittedEvents::Transaction { events, .. } => {
                    for event in events.iter() {
                        if event.stream_id.as_ref() == stream_id {
                            collected_events.push(event.stream_version);
                        }
                    }
                }
            }
        }

        // Should read events from version 10 to 19
        let expected_count = event_count - start_version as usize;
        assert_eq!(collected_events.len(), expected_count);
        assert_eq!(collected_events[0], start_version);
        assert_eq!(
            collected_events[collected_events.len() - 1],
            event_count as u64 - 1
        );
    }

    #[tokio::test]
    async fn test_stream_iter_single_transaction_multiple_streams() {
        let db = create_test_database(SMALL_SEGMENT_SIZE).await;
        let partition_key = Uuid::new_v4();
        let partition_id = 0;

        // Create events for multiple streams in a single transaction
        let events = vec![
            create_test_event("stream-a", partition_key, partition_id),
            create_test_event("stream-b", partition_key, partition_id),
            create_test_event("stream-a", partition_key, partition_id),
            create_test_event("stream-c", partition_key, partition_id),
            create_test_event("stream-a", partition_key, partition_id),
        ];

        append_transaction_events(&db, events, partition_key, partition_id)
            .await
            .expect("failed to append transaction events");

        // Read stream-a events
        let mut stream_iter = db
            .read_stream(partition_id, StreamId::new("stream-a").unwrap(), 0, false)
            .await
            .expect("failed to create stream iterator");

        let mut collected_events = Vec::new();
        while let Some(committed_events) = stream_iter.next().await.expect("read err") {
            match committed_events {
                CommittedEvents::Single(event) => {
                    collected_events.push(event.stream_version);
                }
                CommittedEvents::Transaction { events, .. } => {
                    for event in events.iter() {
                        if event.stream_id.as_ref() == "stream-a" {
                            collected_events.push(event.stream_version);
                        }
                    }
                }
            }
        }

        // Should only see events for stream-a: versions 0, 1, 2
        assert_eq!(collected_events, vec![0, 1, 2]);
    }

    #[tokio::test]
    async fn test_stream_iter_transaction_deduplication() {
        let db = create_test_database(SMALL_SEGMENT_SIZE).await;
        let partition_key = Uuid::new_v4();
        let partition_id = 0;
        let stream_id = "test-stream-dedup";

        // Create multiple events for same stream in single transaction
        let events = vec![
            create_test_event(stream_id, partition_key, partition_id),
            create_test_event(stream_id, partition_key, partition_id),
            create_test_event(stream_id, partition_key, partition_id),
        ];

        append_transaction_events(&db, events, partition_key, partition_id)
            .await
            .expect("failed to append transaction events");

        // Add another single event
        append_events_to_stream(&db, stream_id, 1, partition_key, partition_id)
            .await
            .expect("failed to append single event");

        let mut stream_iter = db
            .read_stream(partition_id, StreamId::new(stream_id).unwrap(), 0, false)
            .await
            .expect("failed to create stream iterator");

        let mut collected_events = Vec::new();
        let mut transaction_count = 0;
        let mut single_event_count = 0;

        while let Some(committed_events) = stream_iter.next().await.expect("read err") {
            match committed_events {
                CommittedEvents::Single(event) => {
                    single_event_count += 1;
                    collected_events.push(event.stream_version);
                }
                CommittedEvents::Transaction { events, .. } => {
                    transaction_count += 1;
                    for event in events.iter() {
                        if event.stream_id.as_ref() == stream_id {
                            collected_events.push(event.stream_version);
                        }
                    }
                }
            }
        }

        // Should have one transaction (with 3 events) and one single event
        assert_eq!(transaction_count, 1, "should have exactly one transaction");
        assert_eq!(
            single_event_count, 1,
            "should have exactly one single event"
        );
        assert_eq!(
            collected_events,
            vec![0, 1, 2, 3],
            "events should be in order with last event being version 3 from separate transaction"
        );
    }

    #[tokio::test]
    async fn test_stream_iter_transactions_across_segments() {
        let db = create_test_database(SMALL_SEGMENT_SIZE).await;
        let partition_key = Uuid::new_v4();
        let partition_id = 0;
        let stream_id = "test-stream-cross-segment";

        // Append some single events first
        append_events_to_stream(&db, stream_id, 10, partition_key, partition_id)
            .await
            .expect("failed to append initial events");

        // Create a large transaction that might span segments
        let mut large_transaction_events = Vec::new();
        for _ in 11..=20 {
            large_transaction_events.push(create_test_event(
                stream_id,
                partition_key,
                partition_id,
            ));
        }

        append_transaction_events(&db, large_transaction_events, partition_key, partition_id)
            .await
            .expect("failed to append large transaction");

        // Append more single events
        append_events_to_stream(&db, stream_id, 5, partition_key, partition_id)
            .await
            .expect("failed to append final events");

        let mut stream_iter = db
            .read_stream(partition_id, StreamId::new(stream_id).unwrap(), 0, false)
            .await
            .expect("failed to create stream iterator");

        let mut collected_events = Vec::new();
        while let Some(committed_events) = stream_iter.next().await.expect("read err") {
            match committed_events {
                CommittedEvents::Single(event) => {
                    collected_events.push(event.stream_version);
                }
                CommittedEvents::Transaction { events, .. } => {
                    for event in events.iter() {
                        if event.stream_id.as_ref() == stream_id {
                            collected_events.push(event.stream_version);
                        }
                    }
                }
            }
        }

        // Should read all events in correct order
        let expected_sequence: Vec<u64> = (0..=24).collect();
        assert_eq!(collected_events, expected_sequence);
    }

    #[tokio::test]
    async fn test_stream_iter_empty_transaction_filtering() {
        let db = create_test_database(SMALL_SEGMENT_SIZE).await;
        let partition_key = Uuid::new_v4();
        let partition_id = 0;

        // Create a transaction with events for different streams (none for our target)
        let events = vec![
            create_test_event("other-stream-1", partition_key, partition_id),
            create_test_event("other-stream-2", partition_key, partition_id),
        ];

        append_transaction_events(&db, events, partition_key, partition_id)
            .await
            .expect("failed to append other stream events");

        // Add one event for our target stream
        append_events_to_stream(&db, "target-stream", 1, partition_key, partition_id)
            .await
            .expect("failed to append target event");

        let mut stream_iter = db
            .read_stream(
                partition_id,
                StreamId::new("target-stream").unwrap(),
                0,
                false,
            )
            .await
            .expect("failed to create stream iterator");

        let mut collected_events = Vec::new();
        while let Some(committed_events) = stream_iter.next().await.expect("read err") {
            match committed_events {
                CommittedEvents::Single(event) => {
                    collected_events.push(event.stream_version);
                }
                CommittedEvents::Transaction { events, .. } => {
                    for event in events.iter() {
                        if event.stream_id.as_ref() == "target-stream" {
                            collected_events.push(event.stream_version);
                        }
                    }
                }
            }
        }

        // Should only get the one event for target-stream
        assert_eq!(collected_events, vec![0]);
    }

    #[tokio::test]
    async fn test_stream_iter_ordering_with_mixed_events() {
        let db = create_test_database(SMALL_SEGMENT_SIZE).await;
        let partition_key = Uuid::new_v4();
        let partition_id = 0;
        let stream_id = "test-stream-ordering";

        // Mix of single events and transactions in specific order
        append_events_to_stream(&db, stream_id, 3, partition_key, partition_id)
            .await
            .expect("failed to append initial events");

        // Transaction with multiple events
        let transaction_events = vec![
            create_test_event(stream_id, partition_key, partition_id),
            create_test_event(stream_id, partition_key, partition_id),
        ];
        append_transaction_events(&db, transaction_events, partition_key, partition_id)
            .await
            .expect("failed to append transaction");

        // More single events
        append_events_to_stream(&db, stream_id, 2, partition_key, partition_id)
            .await
            .expect("failed to append final events");

        let mut stream_iter = db
            .read_stream(partition_id, StreamId::new(stream_id).unwrap(), 0, false)
            .await
            .expect("failed to create stream iterator");

        let mut collected_events = Vec::new();
        while let Some(committed_events) = stream_iter.next().await.expect("read err") {
            match committed_events {
                CommittedEvents::Single(event) => {
                    collected_events.push(event.stream_version);
                }
                CommittedEvents::Transaction { events, .. } => {
                    for event in events.iter() {
                        if event.stream_id.as_ref() == stream_id {
                            collected_events.push(event.stream_version);
                        }
                    }
                }
            }
        }

        // Should read: 1,2,3 (singles), then 4,5 (transaction), then 6,7 (singles)
        let expected = vec![0, 1, 2, 3, 4, 5, 6];
        assert_eq!(collected_events, expected);
    }

    #[tokio::test]
    async fn test_stream_iter_no_gaps_across_segments() {
        let db = create_test_database(SMALL_SEGMENT_SIZE).await;
        let partition_key = Uuid::new_v4();
        let partition_id = 0;
        let stream_id = "test-stream-no-gaps";

        // Append enough events to span multiple segments
        let total_events = 100;
        for _ in 0..10 {
            for _ in 0..10 {
                let event = create_test_event(stream_id, partition_key, partition_id);
                let transaction = Transaction::new(partition_key, partition_id, smallvec![event])
                    .expect("failed to create transaction");
                db.append_events(transaction)
                    .await
                    .expect("failed to append event");
            }
        }

        let mut stream_iter = db
            .read_stream(partition_id, StreamId::new(stream_id).unwrap(), 0, false)
            .await
            .expect("failed to create stream iterator");

        let mut collected_events = Vec::new();
        while let Some(committed_events) = stream_iter.next().await.expect("read err") {
            match committed_events {
                CommittedEvents::Single(event) => {
                    collected_events.push(event.stream_version);
                }
                CommittedEvents::Transaction { events, .. } => {
                    for event in events.iter() {
                        if event.stream_id.as_ref() == stream_id {
                            collected_events.push(event.stream_version);
                        }
                    }
                }
            }
        }

        // Verify no gaps and correct count
        assert_eq!(collected_events.len(), total_events);

        // Check for any gaps or duplicates (0-based versions)
        for (i, &version) in collected_events.iter().enumerate() {
            assert_eq!(
                version, i as u64,
                "gap or duplicate detected: expected={}, actual={}", i, version
            );
        }
    }

    #[tokio::test]
    async fn test_stream_iter_large_stream_completeness() {
        let db = create_test_database(SMALL_SEGMENT_SIZE).await;
        let partition_key = Uuid::new_v4();
        let partition_id = 0;
        let stream_id = "test-stream-large";

        // Create a large stream with mixed single events and transactions
        let mut expected_versions = Vec::new();

        // Add 50 single events (versions 0-49)
        append_events_to_stream(&db, stream_id, 50, partition_key, partition_id)
            .await
            .expect("failed to append single events");
        expected_versions.extend(0..50);

        // Add 10 transactions with 3 events each (versions 50-79)
        for i in 0..10 {
            let start_version = 50 + i * 3;
            let transaction_events = vec![
                create_test_event(stream_id, partition_key, partition_id),
                create_test_event(stream_id, partition_key, partition_id),
                create_test_event(stream_id, partition_key, partition_id),
            ];
            append_transaction_events(&db, transaction_events, partition_key, partition_id)
                .await
                .expect("failed to append transaction");
            expected_versions.extend(start_version..start_version + 3);
        }

        // Add 20 more single events (versions 80-99)
        append_events_to_stream(&db, stream_id, 20, partition_key, partition_id)
            .await
            .expect("failed to append final events");
        expected_versions.extend(80..100);

        // Read all events
        let mut stream_iter = db
            .read_stream(partition_id, StreamId::new(stream_id).unwrap(), 0, false)
            .await
            .expect("failed to create stream iterator");

        let mut collected_events = Vec::new();
        while let Some(committed_events) = stream_iter.next().await.expect("read err") {
            match committed_events {
                CommittedEvents::Single(event) => {
                    collected_events.push(event.stream_version);
                }
                CommittedEvents::Transaction { events, .. } => {
                    for event in events.iter() {
                        if event.stream_id.as_ref() == stream_id {
                            collected_events.push(event.stream_version);
                        }
                    }
                }
            }
        }

        // Verify completeness
        assert_eq!(collected_events.len(), expected_versions.len());
        assert_eq!(collected_events, expected_versions);
    }

    #[tokio::test]
    async fn test_stream_iter_batch_vs_single_consistency() {
        let db = create_test_database(SMALL_SEGMENT_SIZE).await;
        let partition_key = Uuid::new_v4();
        let partition_id = 0;
        let stream_id = "test-stream-consistency";

        // Add mixed events
        append_events_to_stream(&db, stream_id, 10, partition_key, partition_id)
            .await
            .expect("failed to append events");

        let transaction_events = vec![
            create_test_event(stream_id, partition_key, partition_id),
            create_test_event(stream_id, partition_key, partition_id),
        ];
        append_transaction_events(&db, transaction_events, partition_key, partition_id)
            .await
            .expect("failed to append transaction");

        // Read using next() method
        let mut stream_iter1 = db
            .read_stream(partition_id, StreamId::new(stream_id).unwrap(), 0, false)
            .await
            .expect("failed to create stream iterator");

        let mut single_read_events = Vec::new();
        while let Some(committed_events) = stream_iter1.next().await.expect("read err") {
            match committed_events {
                CommittedEvents::Single(event) => {
                    single_read_events.push(event.stream_version);
                }
                CommittedEvents::Transaction { events, .. } => {
                    for event in events.iter() {
                        if event.stream_id.as_ref() == stream_id {
                            single_read_events.push(event.stream_version);
                        }
                    }
                }
            }
        }

        // Read using next_batch() method
        let mut stream_iter2 = db
            .read_stream(partition_id, StreamId::new(stream_id).unwrap(), 0, false)
            .await
            .expect("failed to create stream iterator");

        let mut batch_read_events = Vec::new();
        loop {
            let batch = stream_iter2.next_batch(5).await.expect("read err");
            if batch.is_empty() {
                break;
            }

            for committed_events in batch {
                match committed_events {
                    CommittedEvents::Single(event) => {
                        batch_read_events.push(event.stream_version);
                    }
                    CommittedEvents::Transaction { events, .. } => {
                        for event in events.iter() {
                            if event.stream_id.as_ref() == stream_id {
                                batch_read_events.push(event.stream_version);
                            }
                        }
                    }
                }
            }
        }

        // Both methods should return identical results
        assert_eq!(single_read_events, batch_read_events);
        assert_eq!(single_read_events.len(), 12); // 10 single + 2 from transaction
    }

    #[tokio::test]
    async fn test_stream_iter_cache_behavior() {
        let db = create_test_database(SMALL_SEGMENT_SIZE).await;
        let partition_key = Uuid::new_v4();
        let partition_id = 0;
        let stream_id = "test-stream-cache";

        // Create enough events to force multiple segments and cache operations
        let event_count = 100;
        append_events_to_stream(&db, stream_id, event_count, partition_key, partition_id)
            .await
            .expect("failed to append events");

        // First pass - populate cache
        let mut stream_iter1 = db
            .read_stream(partition_id, StreamId::new(stream_id).unwrap(), 0, false)
            .await
            .expect("failed to create stream iterator");

        let mut first_pass_events = Vec::new();
        while let Some(committed_events) = stream_iter1.next().await.expect("read err") {
            match committed_events {
                CommittedEvents::Single(event) => {
                    first_pass_events.push(event.stream_version);
                }
                CommittedEvents::Transaction { events, .. } => {
                    for event in events.iter() {
                        if event.stream_id.as_ref() == stream_id {
                            first_pass_events.push(event.stream_version);
                        }
                    }
                }
            }
        }

        // Second pass - should use cached data
        let mut stream_iter2 = db
            .read_stream(partition_id, StreamId::new(stream_id).unwrap(), 0, false)
            .await
            .expect("failed to create stream iterator");

        let mut second_pass_events = Vec::new();
        while let Some(committed_events) = stream_iter2.next().await.expect("read err") {
            match committed_events {
                CommittedEvents::Single(event) => {
                    second_pass_events.push(event.stream_version);
                }
                CommittedEvents::Transaction { events, .. } => {
                    for event in events.iter() {
                        if event.stream_id.as_ref() == stream_id {
                            second_pass_events.push(event.stream_version);
                        }
                    }
                }
            }
        }

        // Both passes should return identical results
        assert_eq!(first_pass_events, second_pass_events);
        assert_eq!(first_pass_events.len(), event_count);
    }

    #[tokio::test]
    async fn test_stream_iter_partial_segment_reading() {
        let db = create_test_database(SMALL_SEGMENT_SIZE).await;
        let partition_key = Uuid::new_v4();
        let partition_id = 0;
        let stream_id = "test-stream-partial";

        let event_count = 50;
        append_events_to_stream(&db, stream_id, event_count, partition_key, partition_id)
            .await
            .expect("failed to append events");

        // Read only first 25 events
        let mut stream_iter = db
            .read_stream(partition_id, StreamId::new(stream_id).unwrap(), 0, false)
            .await
            .expect("failed to create stream iterator");

        let mut collected_events = Vec::new();
        let target_count = 25;

        while collected_events.len() < target_count {
            if let Some(committed_events) = stream_iter.next().await.expect("read err") {
                match committed_events {
                    CommittedEvents::Single(event) => {
                        collected_events.push(event.stream_version);
                    }
                    CommittedEvents::Transaction { events, .. } => {
                        for event in events.iter() {
                            if event.stream_id.as_ref() == stream_id {
                                collected_events.push(event.stream_version);
                            }
                        }
                    }
                }
            } else {
                break;
            }
        }

        assert_eq!(collected_events.len(), target_count);
        for (i, &version) in collected_events.iter().enumerate() {
            assert_eq!(version, i as u64);
        }
    }

    #[tokio::test]
    async fn test_stream_iter_empty_stream() {
        let db = create_test_database(SMALL_SEGMENT_SIZE).await;
        let partition_id = 0;
        let stream_id = "non-existent-stream";

        let mut stream_iter = db
            .read_stream(partition_id, StreamId::new(stream_id).unwrap(), 0, false)
            .await
            .expect("failed to create stream iterator");

        let result = stream_iter.next().await.expect("read err");
        assert!(result.is_none(), "empty stream should return None");

        // Test batch reading on empty stream
        let batch = stream_iter.next_batch(10).await.expect("read err");
        assert!(batch.is_empty(), "empty stream should return empty batch");
    }

    #[tokio::test]
    async fn test_stream_iter_start_beyond_stream_end() {
        let db = create_test_database(SMALL_SEGMENT_SIZE).await;
        let partition_key = Uuid::new_v4();
        let partition_id = 0;
        let stream_id = "test-stream-beyond";

        // Add only 5 events
        append_events_to_stream(&db, stream_id, 5, partition_key, partition_id)
            .await
            .expect("failed to append events");

        // Try to start reading from version 10 (beyond stream end)
        let mut stream_iter = db
            .read_stream(partition_id, StreamId::new(stream_id).unwrap(), 10, false)
            .await
            .expect("failed to create stream iterator");

        let result = stream_iter.next().await.expect("read err");
        assert!(
            result.is_none(),
            "reading beyond stream end should return None"
        );
    }

    #[tokio::test]
    async fn test_stream_iter_concurrent_writing_reading() {
        let db = create_test_database(SMALL_SEGMENT_SIZE).await;
        let partition_key = Uuid::new_v4();
        let partition_id = 0;
        let stream_id = "test-stream-concurrent";

        // Add initial events
        append_events_to_stream(&db, stream_id, 10, partition_key, partition_id)
            .await
            .expect("failed to append initial events");

        // Start reading
        let mut stream_iter = db
            .read_stream(partition_id, StreamId::new(stream_id).unwrap(), 0, false)
            .await
            .expect("failed to create stream iterator");

        let mut collected_events = Vec::new();

        // Read first few events
        for _ in 0..5 {
            if let Some(committed_events) = stream_iter.next().await.expect("read err") {
                match committed_events {
                    CommittedEvents::Single(event) => {
                        collected_events.push(event.stream_version);
                    }
                    CommittedEvents::Transaction { events, .. } => {
                        for event in events.iter() {
                            if event.stream_id.as_ref() == stream_id {
                                collected_events.push(event.stream_version);
                            }
                        }
                    }
                }
            }
        }

        // Write more events while reading
        append_events_to_stream(&db, stream_id, 5, partition_key, partition_id)
            .await
            .expect("failed to append additional events");

        // Continue reading
        while let Some(committed_events) = stream_iter.next().await.expect("read err") {
            match committed_events {
                CommittedEvents::Single(event) => {
                    collected_events.push(event.stream_version);
                }
                CommittedEvents::Transaction { events, .. } => {
                    for event in events.iter() {
                        if event.stream_id.as_ref() == stream_id {
                            collected_events.push(event.stream_version);
                        }
                    }
                }
            }
        }

        // Should read initial 10 events, additional 5 may or may not be visible
        // depending on when segments close
        assert!(
            collected_events.len() >= 10,
            "should read at least initial 10 events"
        );
        assert!(
            collected_events.len() <= 15,
            "should read at most 15 events"
        );

        // Events should be in order (0-based)
        for (i, &version) in collected_events.iter().enumerate() {
            if i < 10 {
                assert_eq!(version, i as u64);
            } else {
                // Additional events continue from version 10
                assert_eq!(version, i as u64);
            }
        }
    }

    #[tokio::test]
    async fn test_stream_iter_very_large_batch() {
        let db = create_test_database(SMALL_SEGMENT_SIZE).await;
        let partition_key = Uuid::new_v4();
        let partition_id = 0;
        let stream_id = "test-stream-large-batch";

        let event_count = 20;
        append_events_to_stream(&db, stream_id, event_count, partition_key, partition_id)
            .await
            .expect("failed to append events");

        let mut stream_iter = db
            .read_stream(partition_id, StreamId::new(stream_id).unwrap(), 0, false)
            .await
            .expect("failed to create stream iterator");

        // Request more events than available
        let batch = stream_iter.next_batch(100).await.expect("read err");

        let mut collected_events = Vec::new();
        for committed_events in batch {
            match committed_events {
                CommittedEvents::Single(event) => {
                    collected_events.push(event.stream_version);
                }
                CommittedEvents::Transaction { events, .. } => {
                    for event in events.iter() {
                        if event.stream_id.as_ref() == stream_id {
                            collected_events.push(event.stream_version);
                        }
                    }
                }
            }
        }

        // Should return all available events
        assert_eq!(collected_events.len(), event_count);
        for (i, &version) in collected_events.iter().enumerate() {
            assert_eq!(version, i as u64);
        }

        // Subsequent batch should be empty
        let empty_batch = stream_iter.next_batch(10).await.expect("read err");
        assert!(empty_batch.is_empty());
    }
}
